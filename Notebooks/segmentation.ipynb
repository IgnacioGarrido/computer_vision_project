{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAb7vPI3HBpn",
        "colab_type": "code",
        "outputId": "0d366d47-4edb-40fb-d4ac-48dab6627048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import pickle\n",
        "import cv2\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "import os\n",
        "from skimage.transform import resize\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
        "from keras.layers.core import Dropout\n",
        "from keras import regularizers\n",
        "from keras.models import *\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import time\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import plot_model\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "if not os.path.exists(\"figures\"):\n",
        "  os.mkdir(\"figures\")\n",
        "\n",
        "class LossLog(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_epoch_begin(self,epoch,logs={}):\n",
        "        self.losses_epoch = []\n",
        "\n",
        "    def on_epoch_end(self,epoch,logs={}):\n",
        "        self.losses.append(self.losses_epoch)\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses_epoch.append(logs.get('loss'))\n",
        "\n",
        "    def on_train_end(self,logs={}):\n",
        "        layers=len(self.model.layers)\n",
        "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        save_loss_to_csv(self.losses,layers,timestr)\n",
        "\n",
        "def save_pickle(path, obj):\n",
        "    print('Saving object in: ' + path)\n",
        "    with open(path, 'wb+') as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "def load_pickle(path):\n",
        "    print('Loading object from: ' + path)\n",
        "    with open(path, 'rb') as f:\n",
        "        obj = pickle.load(f)\n",
        "    return obj\n",
        "\n",
        "def save_model(model,filename):\n",
        "    model.save(\"/content/\"+filename+\".h5\")    \n",
        "\n",
        "def display_images(original,reconstructed):\n",
        "    res=np.concatenate((original,reconstructed),axis=1)\n",
        "    plt.imshow(res)\n",
        "    plt.show()\n",
        "    \n",
        "def display_images3(original,y_value,reconstructed, name_model):\n",
        "    f = plt.figure()\n",
        "    f.add_subplot(1,3, 1)\n",
        "    plt.imshow(original)\n",
        "    plt.title(\"Original\")\n",
        "    f.add_subplot(1,3, 2)\n",
        "    plt.imshow(y_value)\n",
        "    plt.title(\"Segmentation\")\n",
        "    f.add_subplot(1,3, 3)\n",
        "    plt.imshow(reconstructed)\n",
        "    plt.title(\"Output\")\n",
        "    plt.show(block=True)\n",
        "    f.savefig(\"figures/\"+name_model+\".png\")\n",
        "    \n",
        "#im_nums: IDs of the images\n",
        "#x_val: Dataset of the original images of validation (RGB)\n",
        "#gray_x_val: Dataset of the original images of validation (grayscale)\n",
        "#y_val: Dataset of the y-values of validation\n",
        "#model: NN that is being analyzed \n",
        "#name_model: name of the file in which it will be stored\n",
        "def displayAndSaveImages3(im_nums,x_val,gray_x_val,y_val,model,name_model):\n",
        "    im_iter = 0\n",
        "    rows = len(im_nums)\n",
        "    f = plt.figure(figsize=(10,int(15*rows/4)))\n",
        "    #f=plt.figure()\n",
        "    for i in range(len(im_nums)):\n",
        "      img_x = gray_x_val[im_nums[i]:(im_nums[i]+1)]\n",
        "      img_y = y_val[im_nums[i]:(im_nums[i]+1)]\n",
        "      img = model.predict(img_x)\n",
        "      original = x_val[im_nums[i]:(im_nums[i]+1)][0]\n",
        "      segmentation = img_y[0][:,:,0]\n",
        "      reconstructed = img[0][:,:,0]      \n",
        "      f.add_subplot(rows,3, int(1 + im_iter*3))\n",
        "      plt.imshow(original)\n",
        "      plt.title(\"Original\")\n",
        "      f.add_subplot(rows,3, int(2 + im_iter*3))\n",
        "      plt.imshow(segmentation)\n",
        "      plt.title(\"Segmentation\")\n",
        "      f.add_subplot(rows,3, int(3 + im_iter*3))\n",
        "      plt.imshow(reconstructed)      \n",
        "      plt.title(\"Output\")\n",
        "      im_iter = im_iter + 1\n",
        "    f.savefig(\"figures/\"+name_model+\"_testFigures.png\")\n",
        "    \n",
        "    \n",
        "def save_loss_to_csv(losses,layers,timestr):\n",
        "    filepath=\"/content/\"+timestr+\"_layers-\"+str(layers)+\".csv\"\n",
        "\n",
        "    with open(filepath, 'w+') as csvFile:\n",
        "        writer = csv.writer(csvFile)\n",
        "        for epoch in losses:\n",
        "            writer.writerow(epoch)\n",
        "\n",
        "    csvFile.close()\n",
        "    \n",
        "    \n",
        "smooth = 1.\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "  \n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "  return -dice_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
        "  # some implementations don't square y_pred\n",
        "  denominator = tf.reduce_sum(y_true + tf.square(y_pred))\n",
        "  return -numerator/(denominator + tf.keras.backend.epsilon())\n",
        "\n",
        "\n",
        "def binary_crossentropy(y_true, y_pred):\n",
        "  return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
        "\n",
        "\n",
        "def loss_funct(y_true, y_pred):\n",
        "  return binary_crossentropy(y_true, y_pred) + (1-dice_coef(y_true, y_pred))\n",
        "\n",
        "\n",
        "def RGB2GRAY(x):\n",
        "  grayImages = np.zeros((x.shape[0],x.shape[1],x.shape[1],1))\n",
        "  for i in range(len(x)):\n",
        "    grayImages[i,:,:,0] = cv2.cvtColor(x[i], cv2.COLOR_RGB2GRAY)\n",
        "  return grayImages\n",
        "\n",
        "image_size = 128\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 6.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 4.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 4.9MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 4.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 4.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 4.9MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnu7kLAmHjfP",
        "colab_type": "text"
      },
      "source": [
        "### ***Load data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rXW5PnkHqVV",
        "colab_type": "code",
        "outputId": "069cc050-2592-4ba8-e551-01de25ab0a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        " # Authenticate and create the PyDrive client.\n",
        " # This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Segmentation all\n",
        "train_file_id = '151xqm5vZwFdAZeqnjc0mt1QJEdyuWPMt'\n",
        "x_train_file = drive.CreateFile({'id': train_file_id})\n",
        "val_file_id = '1fRLHfjhE4GxTCl4mSeL1e3vFrPwVvO98'\n",
        "x_val_file = drive.CreateFile({'id': val_file_id})\n",
        "test_file_id = '1fRLHfjhE4GxTCl4mSeL1e3vFrPwVvO98'\n",
        "x_test_file = drive.CreateFile({'id': test_file_id})\n",
        "\n",
        "y_train_file_id = '10heHA2R1ghKTKkd86QJ_JbBA09Baeomw'\n",
        "y_train_file = drive.CreateFile({'id': y_train_file_id})\n",
        "y_val_file_id = '1VIR0WwZ7Yx15gXaKhZfHoj1QJrOeWpSO'\n",
        "y_val_file = drive.CreateFile({'id': y_val_file_id})\n",
        "y_test_file_id = '1VIR0WwZ7Yx15gXaKhZfHoj1QJrOeWpSO'\n",
        "y_test_file = drive.CreateFile({'id': y_test_file_id})\n",
        "\n",
        "x_train_file.GetContentFile('x_train')\n",
        "x_val_file.GetContentFile('x_val')\n",
        "x_test_file.GetContentFile('x_test')\n",
        "\n",
        "y_train_file.GetContentFile('y_train')\n",
        "y_val_file.GetContentFile('y_val')\n",
        "y_test_file.GetContentFile('y_test')\n",
        "\n",
        "\n",
        "x_train=load_pickle('x_train')\n",
        "y_train=load_pickle(\"y_train\")\n",
        "x_val=load_pickle('x_val')\n",
        "y_val=load_pickle(\"y_val\")\n",
        "x_test=load_pickle('x_test')\n",
        "y_test=load_pickle(\"y_test\")\n",
        "\n",
        "gray_x_train = RGB2GRAY(x_train)\n",
        "gray_x_val = RGB2GRAY(x_val)\n",
        "gray_x_test = RGB2GRAY(x_test)\n",
        "\n",
        "x_train_file = 0\n",
        "x_val_file = 0\n",
        "x_test_file = 0\n",
        "y_train_file = 0\n",
        "y_val_file = 0 \n",
        "y_test_file = 0 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading object from: x_train\n",
            "Loading object from: y_train\n",
            "Loading object from: x_val\n",
            "Loading object from: y_val\n",
            "Loading object from: x_test\n",
            "Loading object from: y_test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMcVJSvCIR_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "#%%  SSSS EEEE GGGG M   M EEEE N  N TTTTT AAAA TTTTT IIIII OOOO N  N                                                                                                  \n",
        "#%%  S    E    G    MM MM E    NN N   T   A  A   T     I   O  O NN N                                                    \n",
        "#%%  SSSS EE   G GG M M M EEE  N NN   T   AAAA   T     I   O  O N NN                                                                      \n",
        "#%%     S E    G  G M   M E    N  N   T   A  A   T     I   O  O N  N                                                                      \n",
        "#%%  SSSS EEEE GGGG M   M EEEE N  N   T   A  A   T   IIIII OOOO N  N                                                                       \n",
        "#%%"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BCwFvcKeN0U",
        "colab_type": "text"
      },
      "source": [
        "## ***Whole autoencoder***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LabGev6lII8b",
        "colab_type": "text"
      },
      "source": [
        "### ***Model definition***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gaG_q8zIS23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Choose loss function\n",
        "#lossFunction = 'binary_crossentropy'\n",
        "lossFunction = dice_coef_loss\n",
        "\n",
        "segment = Sequential()\n",
        "\n",
        "#Down\n",
        "segment.add(Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=(image_size, image_size, 1))) \n",
        "segment.add(BatchNormalization())\n",
        "segment.add(MaxPooling2D((2, 2), padding='same'))  #Reduction in size x2.\n",
        "segment.add(Dropout(0.6))\n",
        "segment.add(Conv2D(32, (3, 3), activation='relu', padding='same')) \n",
        "segment.add(BatchNormalization())\n",
        "segment.add(MaxPooling2D((2, 2), padding='same'))  #Reduction in size x2.\n",
        "segment.add(Dropout(0.6))\n",
        "segment.add(Conv2D(64, (3, 3), activation='relu', padding='same')) \n",
        "segment.add(BatchNormalization())\n",
        "segment.add(MaxPooling2D((2, 2), padding='same'))  #Reduction in size x2.\n",
        "segment.add(Dropout(0.6))\n",
        "#Center\n",
        "segment.add(Conv2D(128, (3, 3), activation='relu', padding='same')) \n",
        "segment.add(BatchNormalization())\n",
        "#Up\n",
        "segment.add(UpSampling2D(size=(2, 2)))\n",
        "segment.add(Conv2D(64, (3, 3), activation='relu', padding='same')) \n",
        "segment.add(BatchNormalization())\n",
        "segment.add(UpSampling2D(size=(2, 2)))\n",
        "segment.add(Conv2D(32, (3, 3), activation='relu', padding='same')) \n",
        "segment.add(BatchNormalization())\n",
        "segment.add(UpSampling2D(size=(2, 2)))\n",
        "segment.add(Conv2D(16, (3, 3), activation='relu', padding='same')) \n",
        "segment.add(BatchNormalization())\n",
        "segment.add(Conv2D(1, (3, 3), activation='sigmoid', padding='same'))\n",
        "\n",
        "segment.summary()\n",
        "  \n",
        "segment.compile(optimizer='adam', loss=lossFunction)\n",
        "\n",
        "callbacks = [\n",
        "ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fg-sODjIaSo",
        "colab_type": "text"
      },
      "source": [
        "### ***Model training***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_xAdPLSIc64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train the network\n",
        "history_segment = segment.fit(gray_x_train, y_train, epochs=150, batch_size=16, validation_data=(gray_x_val, y_val), shuffle = True, callbacks = callbacks)\n",
        "segment.load_weights('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKydeZ-0Gvno",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kER5VOQEG7ta",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK5KZeLaG9pO",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4p391IeGSiO",
        "colab_type": "text"
      },
      "source": [
        "## ***Save model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbRkCEfYDdFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################\n",
        "#Parameters to change:\n",
        "################################################################################\n",
        "#In name_model put the name of the last executed neural network\n",
        "name_model = \"join_binary_crossentropy\"\n",
        "#Load the model\n",
        "model = join_model\n",
        "#Flags\n",
        "Flag_GreedyLayerWise = 0\n",
        "Flag_JoinModel = 1\n",
        "#In history put the name of the history of the NN that you want to save (what is before the fit statement)\n",
        "history = history_join_model\n",
        "################################################################################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z67j1vtOF8nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"figures/\"+name_model):\n",
        "  os.mkdir(\"figures/\"+name_model)\n",
        "  \n",
        "name_model_sub = name_model+\"/\"+name_model \n",
        "file_name = \"figures/\" + name_model_sub + \"_model.png\"\n",
        "plot_model(model, to_file=file_name, show_shapes=True, show_layer_names=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAknmBiMogDM",
        "colab_type": "text"
      },
      "source": [
        "### ***Plot training metrics***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz5bH67CofKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot + save\n",
        "loss_name = \"figures/\" + name_model_sub + \"_lossFunction.png\"\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val', 'best model'], loc='upper right')\n",
        "plt.savefig(loss_name)\n",
        "plt.show()\n",
        "\n",
        "if Flag_GreedyLayerWise:\n",
        "  if not os.path.exists(\"figures/\"+name_model+\"/submodels\"):\n",
        "     os.mkdir(\"figures/\"+name_model+\"/submodels\")\n",
        "  history_models = [history_model1, history_model2, history_model3]\n",
        "  mod_count = 1\n",
        "  for hist in history_models:\n",
        "    history = hist\n",
        "    #Plot + save\n",
        "    loss_name = \"figures/\" + name_model + \"/submodels/submodel_\" + str(mod_count) + \"_lossFunction.png\"\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.plot(np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val', 'best model'], loc='upper right')\n",
        "    plt.savefig(loss_name)\n",
        "    plt.show()\n",
        "    mod_count = mod_count + 1\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXWYj80Qofpj",
        "colab_type": "text"
      },
      "source": [
        "### ***Plot results & save results***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P-Nub3g1c6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_nums= [1,2,10,11]\n",
        "\n",
        "if Flag_JoinModel == 1:\n",
        "  displayAndSaveImages3(im_nums,x_test,join_x_test,y_test,model,name_model_sub)\n",
        "else:\n",
        "  displayAndSaveImages3(im_nums,x_test,gray_x_test,y_test,model,name_model_sub)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "686iHx3pGtzH",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ0oZBkBGtiN",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n83i4O78Gs_W",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHbib3fWh1Xo",
        "colab_type": "text"
      },
      "source": [
        "## ***Greedy layer-wise training***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyzMZnPDh91h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Choose loss function\n",
        "#lossFunction = 'binary_crossentropy'\n",
        "lossFunction = dice_coef_loss\n",
        "\n",
        "callbacks = [ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)]\n",
        "\n",
        "\n",
        "#Model with 3 layers\n",
        "\n",
        "print(\"Model 1 conv...\")\n",
        "#Model 1\n",
        "inputs = Input((128, 128, 1))\n",
        "conv1 = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "pool1 = Dropout(0.6) (pool1)\n",
        "up1 = UpSampling2D(size=(2, 2))(pool1) \n",
        "up1 = Conv2D(16, (3, 3), activation='relu', padding='same')(up1)\n",
        "up1 = BatchNormalization()(up1)\n",
        "up0 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up1)\n",
        "model1 = Model(inputs=[inputs], outputs=[up0])\n",
        "model1.summary()\n",
        "model1.compile(optimizer='adam', loss=lossFunction, metrics=[dice_coef])\n",
        "history_model1 = model1.fit(gray_x_train, y_train, batch_size=16, nb_epoch=20, verbose=1, shuffle=True, validation_data=(gray_x_val, y_val), callbacks = callbacks)\n",
        "model1.load_weights('model.h5')\n",
        "callbacks = [ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)]\n",
        "\n",
        "print(\"Model 2 conv...\")\n",
        "#Model 2\n",
        "conv2 = Conv2D(32, (3, 3), activation='relu', padding='same')(pool1)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "pool2 = Dropout(0.6) (pool2)\n",
        "up2 = UpSampling2D(size=(2, 2))(pool2) \n",
        "up2 = Conv2D(32, (3, 3), activation='relu', padding='same')(up2)\n",
        "up2 = BatchNormalization()(up2)\n",
        "up1 = UpSampling2D(size=(2, 2))(up2) \n",
        "up1 = Conv2D(16, (3, 3), activation='relu', padding='same')(up1)\n",
        "up1 = BatchNormalization()(up1)\n",
        "up0 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up1)\n",
        "model2 = Model(inputs=[inputs], outputs=[up0])\n",
        "model2.layers[1].trainable = False\n",
        "model2.layers[3].trainable = False\n",
        "model2.summary()\n",
        "model2.compile(optimizer='adam', loss=lossFunction, metrics=[dice_coef])\n",
        "history_model2 = model2.fit(gray_x_train, y_train, batch_size=16, nb_epoch=25, verbose=1, shuffle=True, validation_data=(gray_x_val, y_val), callbacks = callbacks)\n",
        "model2.load_weights('model.h5')\n",
        "callbacks = [ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)]\n",
        "\n",
        "\n",
        "print(\"Model 3 conv...\")\n",
        "#Model 3\n",
        "conv3 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool2)\n",
        "conv3 = BatchNormalization()(conv3)\n",
        "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "pool3 = Dropout(0.6) (pool3)\n",
        "up3 = UpSampling2D(size=(2, 2))(pool3) \n",
        "up3 = Conv2D(64, (3, 3), activation='relu', padding='same')(up3)\n",
        "up3 = BatchNormalization()(up3)\n",
        "up2 = UpSampling2D(size=(2, 2))(up3) \n",
        "up2 = Conv2D(32, (3, 3), activation='relu', padding='same')(up2)\n",
        "up2 = BatchNormalization()(up2)\n",
        "up1 = UpSampling2D(size=(2, 2))(up2) \n",
        "up1 = Conv2D(16, (3, 3), activation='relu', padding='same')(up1)\n",
        "up1 = BatchNormalization()(up1)\n",
        "up0 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up1)\n",
        "model3 = Model(inputs=[inputs], outputs=[up0])\n",
        "model3.layers[1].trainable = False\n",
        "model3.layers[3].trainable = False\n",
        "model3.layers[5].trainable = False\n",
        "model3.layers[7].trainable = False\n",
        "model3.summary()\n",
        "model3.compile(optimizer='adam', loss=lossFunction, metrics=[dice_coef])\n",
        "history_model3 = model3.fit(gray_x_train, y_train, batch_size=16, nb_epoch=5, verbose=1, shuffle=True, validation_data=(gray_x_val, y_val), callbacks = callbacks)\n",
        "model3.load_weights('model.h5')\n",
        "callbacks = [ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)]\n",
        "\n",
        "\n",
        "print(\"Fine tunning...\")\n",
        "#Fine tuning\n",
        "#model_end = Model(inputs=[inputs], outputs=[up1])\n",
        "conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool3)\n",
        "conv4 = BatchNormalization()(conv4)\n",
        "up3 = UpSampling2D(size=(2, 2))(conv4) \n",
        "up3 = Conv2D(64, (3, 3), activation='relu', padding='same')(up3)\n",
        "up3 = BatchNormalization()(up3)\n",
        "up2 = UpSampling2D(size=(2, 2))(up3) \n",
        "up2 = Conv2D(32, (3, 3), activation='relu', padding='same')(up2)\n",
        "up2 = BatchNormalization()(up2)\n",
        "up1 = UpSampling2D(size=(2, 2))(up2) \n",
        "up1 = Conv2D(16, (3, 3), activation='relu', padding='same')(up1)\n",
        "up1 = BatchNormalization()(up1)\n",
        "up0 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up1)\n",
        "model_end = Model(inputs=[inputs], outputs=[up0])\n",
        "model_end.layers[1].trainable = True\n",
        "model_end.layers[3].trainable = True\n",
        "model_end.layers[5].trainable = True\n",
        "model_end.layers[7].trainable = True\n",
        "model_end.compile(optimizer='adam', loss=lossFunction, metrics=[dice_coef])\n",
        "model_end.summary()\n",
        "history_model_end = model_end.fit(gray_x_train, y_train, batch_size=16, nb_epoch=100, verbose=1, shuffle=True, validation_data=(gray_x_val, y_val), callbacks = callbacks)\n",
        "model_end.load_weights('model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fqn5Js_EQnx",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJvJjQltEQR5",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmIKsV6UEPt5",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy-Dp8VuNL8K",
        "colab_type": "text"
      },
      "source": [
        "## ***Join both models***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l-J9qV4EMbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Join both outputs:\n",
        "\n",
        "model_binarycrossentropy = keras.models.clone_model(model_end)\n",
        "#model_dicecoef = keras.models.clone_model(model_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePV9-EkaJtIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get the images sets\n",
        "train_binarycrossentropy = model_binarycrossentropy.predict(gray_x_train)\n",
        "train_dicecoef = model_dicecoef.predict(gray_x_train)\n",
        "val_binarycrossentropy = model_binarycrossentropy.predict(gray_x_val)\n",
        "val_dicecoef = model_dicecoef.predict(gray_x_val)\n",
        "\n",
        "join_x_train = np.concatenate((train_binarycrossentropy, train_dicecoef), axis=3)\n",
        "join_x_val = np.concatenate((val_binarycrossentropy, val_dicecoef), axis=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6-xq0XqMOWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Neural network\n",
        "callbacks = [ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)]\n",
        "\n",
        "#Choose loss function\n",
        "lossFunction = 'binary_crossentropy'\n",
        "#lossFunction = dice_coef_loss\n",
        "\n",
        "join_model = Sequential()\n",
        "join_model.add(Conv2D(8, (3, 3), activation='relu', padding='same', input_shape=(image_size, image_size, 2))) \n",
        "join_model.add(Conv2D(1, (3, 3), activation='sigmoid', padding='same'))\n",
        "\n",
        "join_model.summary()\n",
        "  \n",
        "join_model.compile(optimizer='adam', loss=lossFunction)\n",
        "\n",
        "history_join_model = join_model.fit(join_x_train, y_train, epochs=50, batch_size=16, validation_data=(join_x_val, y_val), shuffle = True, callbacks = callbacks)\n",
        "join_model.load_weights('model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEVQo-hd-hSx",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KStUJcwU-esW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get the images sets\n",
        "train_binarycrossentropy = model_binarycrossentropy.predict(gray_x_train)\n",
        "train_dicecoef = model_dicecoef.predict(gray_x_train)\n",
        "val_binarycrossentropy = model_binarycrossentropy.predict(gray_x_val)\n",
        "val_dicecoef = model_dicecoef.predict(gray_x_val)\n",
        "\n",
        "join_x_train = [train_binarycrossentropy, train_dicecoef]\n",
        "join_x_val = [val_binarycrossentropy, val_dicecoef]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfbGD8539P-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Neural network\n",
        "callbacks = [ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)]\n",
        "\n",
        "#Choose loss function\n",
        "lossFunction = 'binary_crossentropy'\n",
        "#lossFunction = dice_coef_loss\n",
        "\n",
        "input1 = Input((128, 128, 1))\n",
        "input2 = Input((128, 128, 1))\n",
        "\n",
        "jn = concatenate([input1,input1], axis = 3)\n",
        "jn = Conv2D(8, (3, 3), activation='relu', padding='same')(jn)\n",
        "jn = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(jn)\n",
        "join_model = Model(inputs=[input1, input2], outputs=[jn])\n",
        "join_model.compile(optimizer='adam', loss=lossFunction)\n",
        "\n",
        "history_join_model = join_model.fit(join_x_train, y_train, epochs=250, batch_size=16, validation_data=(join_x_val, y_val), shuffle = True, callbacks = callbacks)\n",
        "join_model.load_weights('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tExGbWQ3GINg",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeVke_pHGHy9",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_S5yu0rFGpG",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIYA4WWtXWJS",
        "colab_type": "text"
      },
      "source": [
        "## ***Skip connection models***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cONcfGCuWInK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Choose loss function\n",
        "#lossFunction = 'binary_crossentropy'\n",
        "lossFunction = dice_coef_loss\n",
        "\n",
        "callbacks = [ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)]\n",
        "\n",
        "\n",
        "#New model with skip layers:\n",
        "\n",
        "inputs = Input((128, 128, 1))\n",
        "conv1 = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "pool1 = Dropout(0.6) (pool1)\n",
        "conv2 = Conv2D(32, (3, 3), activation='relu', padding='same')(pool1)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "pool2 = Dropout(0.6) (pool2)\n",
        "conv3 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool2)\n",
        "conv3 = BatchNormalization()(conv3)\n",
        "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "pool3 = Dropout(0.6) (pool3)\n",
        "\n",
        "conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool3)\n",
        "conv4 = BatchNormalization()(conv4)\n",
        "\n",
        "up3 = UpSampling2D(size=(2, 2))(conv4) \n",
        "up3 = concatenate([conv3,up3], axis = 3)\n",
        "up3 = Conv2D(64, (3, 3), activation='relu', padding='same')(up3)\n",
        "up3 = BatchNormalization()(up3)\n",
        "up2 = UpSampling2D(size=(2, 2))(up3) \n",
        "up2 = concatenate([conv2,up2], axis = 3)\n",
        "up2 = Conv2D(32, (3, 3), activation='relu', padding='same')(up2)\n",
        "up2 = BatchNormalization()(up2)\n",
        "up1 = UpSampling2D(size=(2, 2))(up2) \n",
        "up1 = concatenate([conv1,up1], axis = 3)\n",
        "up1 = Conv2D(16, (3, 3), activation='relu', padding='same')(up1)\n",
        "up1 = BatchNormalization()(up1)\n",
        "up0 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up1)\n",
        "model_SC = Model(inputs=[inputs], outputs=[up0])\n",
        "\n",
        "model_SC.compile(optimizer='sgd', loss=lossFunction)\n",
        "\n",
        "#model.summary()\n",
        "#plot_model(model, to_file=\"concatenate_model_structure.png\", show_shapes=True, show_layer_names=True)\n",
        "\n",
        "history_model_SC = model_SC.fit(gray_x_train, y_train, batch_size=16, nb_epoch=150, verbose=1, shuffle=True, validation_data=(gray_x_val, y_val), callbacks = callbacks)\n",
        "model_SC.load_weights('model.h5')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5_gMZZzXgSl",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leMfqvmAXgAY",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgwHh3lJXelG",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjWPKUnSMKN3",
        "colab_type": "text"
      },
      "source": [
        "## ***UNET architecture***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBStjRu5E-Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Definition of U-NET\n",
        "\n",
        "def unet_model(lossFunction, pretrained_weights = None,input_size = (128,128,1)):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "    model = Model(input = inputs, output = conv10)\n",
        "\n",
        "    model.compile(optimizer='sgd', loss = lossFunction)\n",
        "    \n",
        "    #model.summary()\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y0tcmfxOcnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Choose loss function\n",
        "#lossFunction = 'binary_crossentropy'\n",
        "lossFunction = dice_coef_loss\n",
        "\n",
        "callbacks = [ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)]\n",
        "unet = unet_model(lossFunction = lossFunction)\n",
        "\n",
        "history_unet = unet.fit(gray_x_train, y_train, epochs=150, batch_size=16, validation_data=(gray_x_val, y_val), shuffle = True, callbacks = callbacks)\n",
        "unet = unet.load_weights('model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apWhy8yjrNYQ",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztu0w2n6rM_v",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPeen-2JrMGu",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mVC-h9AlKOP",
        "colab_type": "text"
      },
      "source": [
        "## ***Download the tests***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQwHT6sarG-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download the figures file\n",
        "from google.colab import files\n",
        "\n",
        "!zip -r /content/figures.zip /content/figures\n",
        "files.download(\"/content/figures.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}